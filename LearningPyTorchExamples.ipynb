{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning PyTorch with Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 99 \n",
      "loss: 782.5994873046875\n",
      "Iteration: 199 \n",
      "loss: 547.6012573242188\n",
      "Iteration: 299 \n",
      "loss: 384.28106689453125\n",
      "Iteration: 399 \n",
      "loss: 270.67474365234375\n",
      "Iteration: 499 \n",
      "loss: 191.58172607421875\n",
      "Iteration: 599 \n",
      "loss: 136.4716033935547\n",
      "Iteration: 699 \n",
      "loss: 98.04182434082031\n",
      "Iteration: 799 \n",
      "loss: 71.22300720214844\n",
      "Iteration: 899 \n",
      "loss: 52.49333190917969\n",
      "Iteration: 999 \n",
      "loss: 39.40369415283203\n",
      "Iteration: 1099 \n",
      "loss: 30.249523162841797\n",
      "Iteration: 1199 \n",
      "loss: 23.843446731567383\n",
      "Iteration: 1299 \n",
      "loss: 19.357749938964844\n",
      "Iteration: 1399 \n",
      "loss: 16.214862823486328\n",
      "Iteration: 1499 \n",
      "loss: 14.011587142944336\n",
      "Iteration: 1599 \n",
      "loss: 12.466182708740234\n",
      "Iteration: 1699 \n",
      "loss: 11.381658554077148\n",
      "Iteration: 1799 \n",
      "loss: 10.620211601257324\n",
      "Iteration: 1899 \n",
      "loss: 10.085333824157715\n",
      "Iteration: 1999 \n",
      "loss: 9.709456443786621\n",
      "Result: y = -0.030294794589281082 + 0.848427414894104x + 0.005226354114711285 x^2 + -0.09214787185192108 x^3\n"
     ]
    }
   ],
   "source": [
    "dtype =  torch.float\n",
    "device  = torch.device(\"cpu\")\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    \n",
    "    #Prediction in the form of a cubic function\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "    \n",
    "    #Loss\n",
    "    loss =  (y_pred - y).pow(2).sum().item()\n",
    "    \n",
    "    #print for every thousand iterations\n",
    "    if t % 100 == 99:\n",
    "        print(\"Iteration:\", t, \"\\nloss:\", loss)\n",
    "        \n",
    "    #Backprop to compute gradients with respect to loss, manually calculated\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "    \n",
    "    #Update weights with gradient descent\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()}x + {c.item()} x^2 + {d.item()} x^3')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple algorithm that utilizes gradient descent to find an approximation to $sin(x)$ with a learned cubic polynomial $a + bx + cx^2 + dx^3$. Notably the mechanics of torch can be utilized to use CPU or GPU acceleration. The data for each of the Torch objects can also be specified.\n",
    "\n",
    "This examples appears to be very similar to numpy except with GPU acceleration and better control of typing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 99 \n",
      "loss: 13373.6513671875\n",
      "Iteration: 199 \n",
      "loss: 13377.8515625\n",
      "Iteration: 299 \n",
      "loss: 13382.1064453125\n",
      "Iteration: 399 \n",
      "loss: 13386.4169921875\n",
      "Iteration: 499 \n",
      "loss: 13390.7822265625\n",
      "Iteration: 599 \n",
      "loss: 13395.203125\n",
      "Iteration: 699 \n",
      "loss: 13399.6787109375\n",
      "Iteration: 799 \n",
      "loss: 13404.2109375\n",
      "Iteration: 899 \n",
      "loss: 13408.794921875\n",
      "Iteration: 999 \n",
      "loss: 13413.4375\n",
      "Iteration: 1099 \n",
      "loss: 13418.1357421875\n",
      "Iteration: 1199 \n",
      "loss: 13422.8876953125\n",
      "Iteration: 1299 \n",
      "loss: 13427.6953125\n",
      "Iteration: 1399 \n",
      "loss: 13432.5576171875\n",
      "Iteration: 1499 \n",
      "loss: 13437.4755859375\n",
      "Iteration: 1599 \n",
      "loss: 13442.44921875\n",
      "Iteration: 1699 \n",
      "loss: 13447.4765625\n",
      "Iteration: 1799 \n",
      "loss: 13452.560546875\n",
      "Iteration: 1899 \n",
      "loss: 13457.697265625\n",
      "Iteration: 1999 \n",
      "loss: 13462.8720703125\n",
      "Result: y = 0.9407525062561035 + 0.027000483125448227x + 0.2496086061000824 x^2 + -0.11617597937583923 x^3\n"
     ]
    }
   ],
   "source": [
    "dtype =  torch.float\n",
    "device  = torch.device(\"cpu\")\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "#Creates size zero tensors, scalars\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    \n",
    "    #Prediction in the form of a cubic function, tensor\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "    \n",
    "    #Loss, tensor\n",
    "    loss =  (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    #print for every thousand iterations\n",
    "    if t % 100 == 99:\n",
    "        print(\"Iteration:\", t, \"\\nloss:\", loss.item())\n",
    "        \n",
    "    #Backprop\n",
    "    loss.backward()\n",
    "    \n",
    "    #Update weights with gradient descent, weights are updated manually\n",
    "    #tracking of the gradient doesn't need to be found here\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * grad_a\n",
    "        b -= learning_rate * grad_b\n",
    "        c -= learning_rate * grad_c\n",
    "        d -= learning_rate * grad_d\n",
    "        \n",
    "        #Set the gradients to zero after updating the weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()}x + {c.item()} x^2 + {d.item()} x^3')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that notable changes include \"requires_grad=True\" a parameter add to the torch objects to  indicate if the tensor, object of torch, needs to have its gradient calculated. This parameter is captured as false by default.\n",
    "\n",
    "The backpropogation of the loss is simply found using the following:\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "This is effective because it uses auto-differentiation after calculating the gradients for a, b, c, and d.\n",
    "\n",
    "A following example where a class for another function to differentiate can be made with \"torch.autograd.Function\" as a subclass of the created function and implementing forward and backward methods for auto-differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 265.669921875\n",
      "199 178.87326049804688\n",
      "299 121.41695404052734\n",
      "399 83.38004302978516\n",
      "499 58.19725799560547\n",
      "599 41.523109436035156\n",
      "699 30.481714248657227\n",
      "799 23.169546127319336\n",
      "899 18.326589584350586\n",
      "999 15.118606567382812\n",
      "1099 12.993467330932617\n",
      "1199 11.585432052612305\n",
      "1299 10.652438163757324\n",
      "1399 10.03408145904541\n",
      "1499 9.624232292175293\n",
      "1599 9.352520942687988\n",
      "1699 9.172361373901367\n",
      "1799 9.052884101867676\n",
      "1899 8.97361946105957\n",
      "1999 8.921043395996094\n",
      "Result: y = 0.0026803219225257635 + 0.8471440672874451 x + -0.0004624009889084846 x^2 + -0.09196533262729645 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
    "# we can consider it as a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
    "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
    "# of shape (2000, 3) \n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. The Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n",
    "# to match the shape of `y`.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p = torch.tensor([1,2,3])\n",
    "\n",
    "This represents the powers for the tensor xx, which is just a transformation of the data stored in x.\n",
    "\n",
    "<p style=\"text-align: center;\">xx = x.unsqueeze(-1).pow(p)</p>\n",
    "\n",
    "The unsqueeze method is used to change the dimensions of the tensor that is being past. Without the unsqueeze method the data will resemble size [2000] rather than size [2000, 1]. This is a necessary transformation because of the application of p.\n",
    "\n",
    "<p style=\"text-align: center;\">model = torch.nn.Sequential(<br>\n",
    "    torch.nn.Linear(3, 1),<br>\n",
    "    torch.nn.Flatten(0, 1)<br>\n",
    ")</p>\n",
    "\n",
    "The previous code describes the construction of a model with 3 inputs parameters(x^1, x^2, x^3) and one output. \"torch.nn.Sequential\" is a container that applies operations in the order that it is past to the constructor. The Linear layer describes a single hidden layer that is takes in the input parameters and outputs a tensor for the approximator. The \"torch.nn.Flatten\" command is utilized to create a value that matches the shape of the true solution, y.\n",
    "\n",
    "<p style=\"text-align: center;\">loss_fn = torch.nn.MSELoss(reduction='sum')<br>\n",
    "    loss = loss_fn(y_pred, y)</p>\n",
    "\n",
    "The previous section of code describes the computation of the loss function using the predicted data and the true solution, y.\n",
    "\n",
    "This section of code represents changes iteratively to weights utilizing the gradient:\n",
    "<p style=\"text-align: center;\">with torch.no_grad():<br>\n",
    "        for param in model.parameters():<br>\n",
    "            param -= learning_rate * param.grad</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
